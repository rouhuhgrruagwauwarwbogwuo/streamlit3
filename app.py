import cv2
import numpy as np
from PIL import Image
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
import random

# ğŸ”¹ é è™•ç†åœ–ç‰‡ï¼Œç¢ºä¿ ResNet å’Œ è‡ªè¨‚ CNN éƒ½èƒ½è™•ç†
def preprocess_for_both_models(img):
    # 1ï¸âƒ£ **é«˜æ¸…åœ–è™•ç†ï¼šLANCZOS ç¸®åœ–**
    img = img.resize((256, 256), Image.Resampling.LANCZOS)

    # 2ï¸âƒ£ **ResNet50 å¿…é ˆ 224x224**
    img = center_crop(img, (224, 224))

    img_array = np.array(img)  # è½‰ç‚º numpy array

    # 3ï¸âƒ£ **å¯é¸ï¼šå° ResNet50 åš Gaussian Blur**
    apply_blur = True  # ğŸš€ é€™è£¡å¯ä»¥é–‹é—œ
    if apply_blur:
        img_array = cv2.GaussianBlur(img_array, (3, 3), 0)

    # 4ï¸âƒ£ **é€²è¡Œé¡è‰²ç©ºé–“è½‰æ›ï¼šè½‰æ›åˆ° YCbCr æˆ– Lab ç©ºé–“**
    apply_color_space_conversion = True
    if apply_color_space_conversion:
        img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2YCrCb)  # è½‰æ›ç‚º YCbCr è‰²å½©ç©ºé–“
        # img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2Lab)  # å¯é¸ï¼šè½‰æ›ç‚º Lab è‰²å½©ç©ºé–“

    # 5ï¸âƒ£ **æ‡‰ç”¨ CLAHEï¼ˆå°æ¯”åº¦å—é™è‡ªé©æ‡‰ç›´æ–¹åœ–å‡è¡¡åŒ–ï¼‰**
    apply_clahe = True
    if apply_clahe:
        lab = cv2.cvtColor(img_array, cv2.COLOR_RGB2Lab)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        img_array = cv2.cvtColor(limg, cv2.COLOR_Lab2RGB)

    # 6ï¸âƒ£ **éš¨æ©Ÿæ—‹è½‰æˆ–ç¸®æ”¾åœ–ç‰‡ï¼ˆæ•¸æ“šå¢å¼·ï¼‰**
    random_flip = random.choice([True, False])
    if random_flip:
        img_array = np.fliplr(img_array)  # éš¨æ©Ÿå·¦å³ç¿»è½‰

    random_rotate = random.choice([True, False])
    if random_rotate:
        angle = random.randint(-10, 10)
        height, width = img_array.shape[:2]
        matrix = cv2.getRotationMatrix2D((width // 2, height // 2), angle, 1)
        img_array = cv2.warpAffine(img_array, matrix, (width, height))

    # 7ï¸âƒ£ **ResNet50 ç‰¹å®šé è™•ç†**
    resnet_input = preprocess_input(np.expand_dims(img_array, axis=0))

    # 8ï¸âƒ£ **è‡ªè¨‚ CNN æ­£è¦åŒ– (0~1)**
    custom_input = np.expand_dims(img_array / 255.0, axis=0)

    return resnet_input, custom_input

# ğŸ”¹ ä¸­å¿ƒè£å‰ªå‡½æ•¸ï¼šå°åœ–ç‰‡é€²è¡Œè£å‰ªï¼Œä½¿å…¶ç¬¦åˆç›®æ¨™å¤§å°
def center_crop(img, target_size):
    width, height = img.size
    target_width, target_height = target_size

    left = (width - target_width) // 2
    top = (height - target_height) // 2
    right = (width + target_width) // 2
    bottom = (height + target_height) // 2

    img = img.crop((left, top, right, bottom))
    return img

# ğŸ”¹ ä½¿ç”¨ ResNet50 æ¨¡å‹é€²è¡Œé æ¸¬
def predict_with_resnet(img):
    # é è™•ç†åœ–ç‰‡
    resnet_input, _ = preprocess_for_both_models(img)

    # è¼‰å…¥ ResNet50 æ¨¡å‹
    resnet_model = ResNet50(weights='imagenet')

    # é æ¸¬çµæœ
    predictions = resnet_model.predict(resnet_input)

    # å°‡é æ¸¬çµæœè½‰ç‚ºæ¨™ç±¤èˆ‡ä¿¡å¿ƒåˆ†æ•¸
    label = np.argmax(predictions)
    confidence = predictions[0][label]

    return label, confidence

# ğŸ”¹ å½±ç‰‡åµæ¸¬ï¼šé€å¹€è™•ç†å½±ç‰‡ä¸¦é¡¯ç¤ºçµæœ
def process_video(input_video_path, output_video_path):
    # è®€å–å½±ç‰‡
    cap = cv2.VideoCapture(input_video_path)

    # å½±ç‰‡ç·¨ç¢¼å™¨è¨­ç½®
    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (640, 480))

    while(cap.isOpened()):
        ret, frame = cap.read()

        if not ret:
            break

        # å°‡ç•¶å‰å¹€è½‰ç‚º PIL å½±åƒæ ¼å¼
        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        # ä½¿ç”¨ ResNet50 é€²è¡Œé æ¸¬
        label, confidence = predict_with_resnet(img)

        # é¡¯ç¤ºçµæœæ–‡å­—
        label_text = 'Deepfake' if label == 1 else 'Real'
        confidence_text = f'{confidence * 100:.2f}%'

        # åœ¨ç•«é¢ä¸Šç¹ªè£½çµæœ
        cv2.putText(frame, f'{label_text} - {confidence_text}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

        # é¡¯ç¤ºè™•ç†å¾Œçš„å¹€
        out.write(frame)  # ä¿å­˜è™•ç†éçš„å¹€
        cv2.imshow('frame', frame)  # é¡¯ç¤ºç•¶å‰å¹€

        # è‹¥æŒ‰ä¸‹ 'q' éµï¼Œé€€å‡ºè™•ç†
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()

# ğŸ”¹ æ¸¬è©¦å½±ç‰‡åµæ¸¬
if __name__ == "__main__":
    input_video_path = "input_video.mp4"  # è«‹æ›¿æ›ç‚ºè¼¸å…¥å½±ç‰‡æª”æ¡ˆè·¯å¾‘
    output_video_path = "output_video.avi"  # è«‹æ›¿æ›ç‚ºè¼¸å‡ºå½±ç‰‡æª”æ¡ˆè·¯å¾‘

    # è™•ç†å½±ç‰‡ä¸¦ä¿å­˜çµæœ
    process_video(input_video_path, output_video_path)
