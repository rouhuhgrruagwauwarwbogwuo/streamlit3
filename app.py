import streamlit as st
import numpy as np
import os
import requests
from PIL import Image
import cv2
import tempfile
from keras.applications import ResNet50
from keras.models import Sequential
from keras.layers import Dense
from keras.preprocessing.image import ImageDataGenerator
from mtcnn import MTCNN
from tensorflow.keras.optimizers import Adam

# â¬‡ï¸ ä¸‹è¼‰æ¨¡å‹ï¼ˆå¦‚æœé‚„æ²’ä¸‹è¼‰ï¼‰
def download_model():
    model_url = "https://huggingface.co/wuwuwu123123/deepfakemodel2/resolve/main/deepfake_cnn_model.h5"
    model_filename = "deepfake_cnn_model.h5"
    if not os.path.exists(model_filename):
        response = requests.get(model_url)
        if response.status_code == 200:
            with open(model_filename, "wb") as f:
                f.write(response.content)
            print("æ¨¡å‹ä¸‹è¼‰æˆåŠŸï¼")
        else:
            print(f"ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            return None
    return model_filename

# âœ… è¼‰å…¥ ResNet50 æ¨¡å‹
try:
    resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    resnet_classifier = Sequential([
        resnet_model,
        Dense(1, activation='sigmoid')
    ])

    # è§£å‡ ResNet50 æ¨¡å‹çš„æœ€å¾Œå¹¾å±¤é€²è¡Œå¾®èª¿
    for layer in resnet_model.layers[:-4]:  # ä¿ç•™ ResNet50 é ‚å±¤ï¼Œè§£å‡å…¶é¤˜éƒ¨åˆ†
        layer.trainable = False
    for layer in resnet_model.layers[-4:]:  # å¾®èª¿æœ€å¾Œå¹¾å±¤
        layer.trainable = True

    resnet_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    print("ResNet50 æ¨¡å‹å·²è¼‰å…¥ä¸¦å¾®èª¿")
except Exception as e:
    print(f"ResNet50 è¼‰å…¥éŒ¯èª¤ï¼š{e}")
    resnet_classifier = None

# âœ… MTCNN åˆå§‹åŒ–
detector = MTCNN()

# âœ… æ“·å–äººè‡‰
def extract_face(pil_img):
    img_array = np.array(pil_img)
    faces = detector.detect_faces(img_array)
    if len(faces) > 0:
        x, y, w, h = faces[0]['box']
        face = img_array[y:y+h, x:x+w]
        return Image.fromarray(face)
    return None

# âœ… åœ–ç‰‡è™•ç†æ–¹æ³•
def apply_clahe(img):
    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])
    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)

def sharpen_image(img):
    kernel = np.array([[0, -1, 0],
                       [-1, 5,-1],
                       [0, -1, 0]])
    return cv2.filter2D(img, -1, kernel)

def high_pass_filter(img):
    blurred = cv2.GaussianBlur(img, (5,5), 0)
    return cv2.subtract(img, blurred)

# âœ… ä¸­å¿ƒè£åˆ‡
def center_crop(img, target_size=(224, 224)):
    width, height = img.size
    new_width, new_height = target_size
    left = (width - new_width) // 2
    top = (height - new_height) // 2
    return img.crop((left, top, left + new_width, top + new_height))

# âœ… é è™•ç†
def preprocess_for_resnet(pil_img):
    img = center_crop(pil_img, (224, 224))
    img_array = np.array(img)

    # âœ… åœ–åƒå¢å¼·
    img_array = apply_clahe(img_array)
    img_array = sharpen_image(img_array)
    img_array = high_pass_filter(img_array)

    img_array = img_array.astype(np.float32) / 255.0
    return np.expand_dims(img_array, axis=0)

# âœ… é æ¸¬
def predict_with_resnet(img):
    resnet_input = preprocess_for_resnet(img)
    pred = resnet_classifier.predict(resnet_input, verbose=0)[0][0]
    label = "Deepfake" if pred > 0.5 else "Real"
    return label, pred

# âœ… é¡¯ç¤ºé æ¸¬
def show_prediction(img):
    label, confidence = predict_with_resnet(img)
    st.image(img, caption="è¼¸å…¥åœ–ç‰‡", use_container_width=True)
    st.subheader(f"ResNet50 åˆ¤æ–·ï¼š**{label}**ï¼ˆä¿¡å¿ƒåº¦ï¼š{confidence:.2%}ï¼‰")

# âœ… è³‡æ–™å¢å¼·è¨­ç½®ï¼ˆå¦‚æœéœ€è¦é€²è¡Œè¨“ç·´ï¼‰
datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# âœ… Streamlit UI
st.set_page_config(page_title="Deepfake åµæ¸¬å™¨", layout="wide")
st.title("ğŸ§  Deepfake åœ–ç‰‡åµæ¸¬å™¨")

tab1, tab2 = st.tabs(["ğŸ–¼ï¸ åœ–ç‰‡åµæ¸¬", "ğŸ¥ å½±ç‰‡åµæ¸¬"])

# âœ… åœ–ç‰‡åµæ¸¬
with tab1:
    st.header("ä¸Šå‚³åœ–ç‰‡é€²è¡Œ Deepfake åµæ¸¬")
    uploaded_image = st.file_uploader("è«‹é¸æ“‡åœ–ç‰‡", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        pil_img = Image.open(uploaded_image).convert("RGB")
        st.image(pil_img, caption="åŸå§‹åœ–ç‰‡", use_container_width=True)

        face_img = extract_face(pil_img)
        if face_img:
            st.image(face_img, caption="åµæ¸¬åˆ°äººè‡‰", width=300)
            show_prediction(face_img)
        else:
            st.info("âš ï¸ æœªåµæ¸¬åˆ°äººè‡‰ï¼Œå°‡ä½¿ç”¨æ•´å¼µåœ–ç‰‡é€²è¡Œé æ¸¬")
            show_prediction(pil_img)

# âœ… å½±ç‰‡åµæ¸¬ï¼ˆåƒ…æ“·å–å‰å¹¾å¹€ï¼‰
with tab2:
    st.header("å½±ç‰‡åµæ¸¬ï¼ˆåƒ…æ“·å–å‰å¹¾å¹€é€²è¡Œåˆ¤æ–·ï¼‰")
    uploaded_video = st.file_uploader("è«‹ä¸Šå‚³å½±ç‰‡", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_video.read())
            video_path = tmp.name

        import cv2
        st.info("ğŸ¬ æ“·å–å¹€èˆ‡åˆ†æä¸­...")
        cap = cv2.VideoCapture(video_path)
        frame_idx = 0
        shown = False

        while cap.isOpened() and not shown:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_idx % 10 == 0:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_frame = Image.fromarray(rgb)
                face_img = extract_face(pil_frame)
                if face_img:
                    st.image(face_img, caption=f"ç¬¬ {frame_idx} å¹€åµæ¸¬åˆ°äººè‡‰", width=300)
                    show_prediction(face_img)
                    shown = True
            frame_idx += 1

        cap.release()
