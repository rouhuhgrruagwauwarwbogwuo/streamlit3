import streamlit as st
import numpy as np
import os
import requests
from PIL import Image
import cv2
import tempfile
from keras.applications import ResNet50, EfficientNetB0, Xception
from keras.models import Sequential
from keras.layers import Dense
from keras.applications.resnet50 import preprocess_input as preprocess_resnet
from keras.applications.efficientnet import preprocess_input as preprocess_efficientnet
from keras.applications.xception import preprocess_input as preprocess_xception
from mtcnn import MTCNN
import matplotlib.pyplot as plt
from keras.preprocessing.image import ImageDataGenerator

# åˆå§‹åŒ– MTCNN
st.set_page_config(page_title="Deepfake åµæ¸¬å™¨", layout="wide")
st.title("ğŸ§  Deepfake åœ–åƒåµæ¸¬å™¨")
detector = MTCNN()

# è¼‰å…¥æ¨¡å‹
@st.cache_resource
def load_models():
    resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    xception_model = Xception(weights='imagenet', include_top=False, pooling='avg', input_shape=(299, 299, 3))

    resnet_classifier = Sequential([resnet_model, Dense(1, activation='sigmoid')])
    efficientnet_classifier = Sequential([efficientnet_model, Dense(1, activation='sigmoid')])
    xception_classifier = Sequential([xception_model, Dense(1, activation='sigmoid')])

    return {
        'ResNet50': resnet_classifier,
        'EfficientNet': efficientnet_classifier,
        'Xception': xception_classifier
    }

# æå–äººè‡‰
@st.cache_data(show_spinner=False)
def extract_face(pil_img):
    img_array = np.array(pil_img)
    faces = detector.detect_faces(img_array)
    if faces:
        x, y, w, h = faces[0]['box']
        face = img_array[y:y+h, x:x+w]
        return Image.fromarray(face)
    return None

# é«˜é€šæ¿¾æ³¢ (å¼·åŒ–é‚Šç·£)
def high_pass_filter(img):
    img_np = np.array(img)
    kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])  # é«˜é€šæ¿¾æ³¢æ ¸
    filtered_img = cv2.filter2D(img_np, -1, kernel)
    return Image.fromarray(filtered_img)

# å¢åŠ æ•¸æ“šå¢å¼·
def augment_image(img):
    datagen = ImageDataGenerator(
        rotation_range=30,  # éš¨æ©Ÿæ—‹è½‰
        width_shift_range=0.2,  # éš¨æ©Ÿæ°´å¹³å¹³ç§»
        height_shift_range=0.2,  # éš¨æ©Ÿå‚ç›´å¹³ç§»
        shear_range=0.2,  # éš¨æ©Ÿå‰ªåˆ‡è®Šæ›
        zoom_range=0.2,  # éš¨æ©Ÿç¸®æ”¾
        horizontal_flip=True,  # éš¨æ©Ÿæ°´å¹³ç¿»è½‰
        fill_mode='nearest'  # å¡«è£œæ¨¡å¼
    )
    
    img_array = np.array(img).reshape((1, ) + np.array(img).shape)
    augmented_img = next(datagen.flow(img_array, batch_size=1))
    return Image.fromarray(augmented_img[0].astype(np.uint8))

# é è™•ç†å„ªåŒ–ï¼šCLAHE + éŠ³åŒ–
def apply_clahe_sharpen(img):
    img_np = np.array(img)
    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    cl = clahe.apply(l)
    lab = cv2.merge((cl, a, b))
    img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

    # éŠ³åŒ–
    blurred = cv2.GaussianBlur(img_clahe, (0, 0), 3)
    sharpened = cv2.addWeighted(img_clahe, 1.5, blurred, -0.5, 0)
    return Image.fromarray(sharpened)

# é è™•ç†åœ–åƒ
def preprocess_image(img, model_name):
    img = apply_clahe_sharpen(img)  # é è™•ç†å„ªåŒ–åŠ å…¥æ­¤è¡Œ
    img = high_pass_filter(img)  # åŠ å…¥é«˜é€šæ¿¾æ³¢

    if model_name == 'Xception':
        img = img.resize((299, 299))
        img_array = np.array(img).astype(np.float32)
        return preprocess_xception(img_array)
    else:
        img = img.resize((224, 224))
        img_array = np.array(img).astype(np.float32)
        if model_name == 'ResNet50':
            return preprocess_resnet(img_array)
        elif model_name == 'EfficientNet':
            return preprocess_efficientnet(img_array)
    return img_array

# å–®æ¨¡å‹é æ¸¬
def predict_model(models, img):
    predictions = []
    for name, model in models.items():
        input_data = preprocess_image(img, name)
        prediction = model.predict(np.expand_dims(input_data, axis=0), verbose=0)
        predictions.append(prediction[0][0])
    return predictions

# é›†æˆé æ¸¬ï¼ˆç°¡å–®å¹³å‡ï¼‰
def stacking_predict(models, img):
    preds = predict_model(models, img)
    avg = np.mean(preds)
    return "Deepfake" if avg > 0.5 else "Real", avg

# é¡¯ç¤ºé æ¸¬çµæœ
def show_prediction(img, models):
    label, confidence = stacking_predict(models, img)
    st.image(img, caption="è¼¸å…¥åœ–åƒ", use_container_width=True)
    st.subheader(f"é æ¸¬çµæœï¼š**{label}**")
    st.markdown(f"ä¿¡å¿ƒåˆ†æ•¸ï¼š**{confidence:.2f}**")

    # é¡¯ç¤ºä¿¡å¿ƒåˆ†æ•¸æ¢
    fig, ax = plt.subplots(figsize=(6, 1))
    ax.barh([0], confidence, color='green' if label == "Real" else 'red')
    ax.set_xlim(0, 1)
    ax.set_yticks([])
    ax.set_xlabel('ä¿¡å¿ƒåˆ†æ•¸')
    st.pyplot(fig)

# UI Tab
models = load_models()
tab1, tab2 = st.tabs(["ğŸ–¼ï¸ åœ–åƒåµæ¸¬", "ğŸ¥ å½±ç‰‡åµæ¸¬"])

with tab1:
    st.header("ä¸Šå‚³åœ–åƒé€²è¡Œ Deepfake åµæ¸¬")
    uploaded_image = st.file_uploader("é¸æ“‡ä¸€å¼µåœ–åƒ", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        pil_img = Image.open(uploaded_image).convert("RGB")
        st.image(pil_img, caption="åŸå§‹åœ–åƒ", use_container_width=True)

        face_img = extract_face(pil_img)
        if face_img:
            st.image(face_img, caption="åµæ¸¬åˆ°äººè‡‰", width=300)
            show_prediction(face_img, models)
        else:
            st.info("âš ï¸ æ²’åµæ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´å¼µåœ–åƒé æ¸¬")
            show_prediction(pil_img, models)

with tab2:
    st.header("å½±ç‰‡åµæ¸¬ï¼ˆè™•ç†å‰å¹¾å¹€ï¼‰")
    uploaded_video = st.file_uploader("é¸æ“‡ä¸€æ®µå½±ç‰‡", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_video.read())
            video_path = tmp.name

        st.info("ğŸ¬ æ­£åœ¨åˆ†æå½±ç‰‡...ï¼ˆå–å‰ 10 å¹€ï¼‰")
        cap = cv2.VideoCapture(video_path)
        frame_idx = 0
        frame_confidences = []
        max_frames = 10
        shown = False  # Initialize shown outside the loop

        while cap.isOpened() and frame_idx < max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_idx % 3 == 0:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_frame = Image.fromarray(rgb)
                face_img = extract_face(pil_frame)
                if face_img:
                    st.image(face_img, caption=f"ç¬¬ {frame_idx} å¹€äººè‡‰", width=300)
                    label, confidence = stacking_predict(models, face_img)
                    st.subheader(f"é æ¸¬çµæœï¼š**{label}**")
                    frame_confidences.append(confidence)
                    shown = True  # Update shown when a frame is processed
                    if len(frame_confidences) == 10:
                        avg_confidence = np.mean(frame_confidences)
                        st.markdown(f"å½±ç‰‡ç¸½é«”ä¿¡å¿ƒåˆ†æ•¸ï¼š**{avg_confidence:.2f}**")
                        break
            frame_idx += 1

        cap.release()

        if not shown:  # Check if no frames with faces were shown
            st.warning("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°äººè‡‰ï¼Œç„¡æ³•é€²è¡Œå½±ç‰‡åˆ†æ")
