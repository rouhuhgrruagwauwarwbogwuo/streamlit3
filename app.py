import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
import requests
from io import BytesIO
from mtcnn import MTCNN  # ä½¿ç”¨ MTCNN é€²è¡Œäººè‡‰åµæ¸¬

# ğŸ”¹ å¾ Hugging Face ä¸‹è¼‰æ¨¡å‹
model_url = "https://huggingface.co/wuwuwu123123/deepfakemodel2/resolve/main/deepfake_cnn_model.h5"
response = requests.get(model_url)

# å°‡æ¨¡å‹å¾ URL ä¸‹è¼‰ä¸¦åŠ è¼‰
model_path = '/tmp/deepfake_cnn_model.h5'
with open(model_path, 'wb') as f:
    f.write(response.content)

# è¼‰å…¥è‡ªè¨‚ CNN æ¨¡å‹
custom_model = load_model(model_path)

# ğŸ”¹ è¼‰å…¥ ResNet50 æ¨¡å‹
resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(256, 256, 3))
resnet_classifier = Sequential([
    resnet_model,
    Dense(1, activation='sigmoid')  # 1 å€‹è¼¸å‡ºç¯€é»ï¼ˆ0: çœŸå¯¦, 1: å‡ï¼‰
])
resnet_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ğŸ”¹ é«˜é€šæ¿¾æ³¢
def apply_highpass_filter(image):
    gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    highpass = cv2.Laplacian(gray_img, cv2.CV_64F)
    return highpass

# ğŸ”¹ CLAHE (Contrast Limited Adaptive Histogram Equalization)
def apply_clahe(image):
    gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    img_clahe = clahe.apply(gray_img)
    return cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB)

# ğŸ”¹ é¡è‰²ç©ºé–“è½‰æ› (YCbCr)
def convert_to_ycbcr(image):
    ycbcr_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)
    return ycbcr_image

# ğŸ”¹ éŠ³åŒ–è™•ç†
def sharpen_image(image):
    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])
    sharpened = cv2.filter2D(image, -1, kernel)
    return sharpened

# ğŸ”¹ é è™•ç†åœ–ç‰‡ï¼Œç¢ºä¿ ResNet å’Œ è‡ªè¨‚ CNN éƒ½èƒ½è™•ç†
def preprocess_for_both_models(image_path):
    img = image.load_img(image_path, target_size=(256, 256))  # èª¿æ•´å¤§å°
    img_array = image.img_to_array(img)
    
    # CLAHE + éŠ³åŒ–
    img_clahe = apply_clahe(img_array)
    img_sharpened = sharpen_image(img_clahe)
    
    # ResNet50 éœ€è¦ç‰¹åˆ¥çš„ preprocess_input
    resnet_input = preprocess_input(np.expand_dims(img_sharpened, axis=0))
    
    # è‡ªè¨‚ CNN åªéœ€è¦æ­£è¦åŒ– (0~1)
    custom_input = np.expand_dims(img_sharpened / 255.0, axis=0)
    
    return resnet_input, custom_input

# ğŸ”¹ ä½¿ç”¨ MTCNN åµæ¸¬äººè‡‰
def extract_face(image):
    detector = MTCNN()
    faces = detector.detect_faces(image)
    
    if len(faces) > 0:
        x, y, w, h = faces[0]['box']
        face_img = image[y:y+h, x:x+w]
        return face_img
    else:
        return None

# ğŸ”¹ é€²è¡Œé æ¸¬
def predict_with_both_models(image_path):
    resnet_input, custom_input = preprocess_for_both_models(image_path)
    
    # ResNet50 é æ¸¬
    resnet_prediction = resnet_classifier.predict(resnet_input)[0][0]
    resnet_label = "å½é€ " if resnet_prediction > 0.5 else "çœŸå¯¦"
    
    # è‡ªè¨‚ CNN æ¨¡å‹é æ¸¬
    custom_prediction = custom_model.predict(custom_input)[0][0]
    custom_label = "å½é€ " if custom_prediction > 0.5 else "çœŸå¯¦"
    
    return resnet_label, resnet_prediction, custom_label, custom_prediction

# ğŸ”¹ é¡¯ç¤ºåœ–ç‰‡å’Œé æ¸¬çµæœ
def show_prediction(image_path):
    # å˜—è©¦æ“·å–äººè‡‰
    img = cv2.imread(image_path)
    face_img = extract_face(img)
    
    if face_img is not None:
        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # è½‰ç‚º RGB æ ¼å¼
        resnet_label, resnet_confidence, custom_label, custom_confidence = predict_with_both_models(face_img)
    else:
        resnet_label, resnet_confidence, custom_label, custom_confidence = predict_with_both_models(image_path)
    
    # é¡¯ç¤ºåœ–ç‰‡
    img = image.load_img(image_path, target_size=(256, 256))
    plt.imshow(img)
    plt.axis('off')  # éš±è—åº§æ¨™è»¸
    
    # é¡¯ç¤ºé æ¸¬çµæœ
    plt.title(f"ResNet50: {resnet_label} ({resnet_confidence:.2%})\n"
              f"Custom CNN: {custom_label} ({custom_confidence:.2%})")
    plt.show()

# ğŸ”¹ é€å¹€è™•ç†å½±ç‰‡
def process_video(video_path):
    cap = cv2.VideoCapture(video_path)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # è™•ç†æ¯ä¸€å¹€
        face_img = extract_face(frame)
        if face_img is not None:
            face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # è½‰ç‚º RGB æ ¼å¼
            resnet_label, resnet_confidence, custom_label, custom_confidence = predict_with_both_models(face_img)
        else:
            resnet_label, resnet_confidence, custom_label, custom_confidence = predict_with_both_models(frame)
        
        # é¡¯ç¤ºé æ¸¬çµæœæ–¼æ¯ä¸€å¹€
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(frame, f"ResNet50: {resnet_label} ({resnet_confidence:.2%})", (10, 30), font, 1, (0, 255, 0), 2)
        cv2.putText(frame, f"Custom CNN: {custom_label} ({custom_confidence:.2%})", (10, 70), font, 1, (0, 255, 0), 2)
        
        # é¡¯ç¤ºè™•ç†å¾Œçš„å¹€
        cv2.imshow('Deepfake Detection', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):  # æŒ‰ 'q' åœæ­¢
            break
    
    cap.release()
    cv2.destroyAllWindows()

# ğŸ”¹ ä½¿ç”¨å½±ç‰‡é€²è¡Œé æ¸¬
video_path = 'test_video.mp4'  # æ›¿æ›æˆæ‚¨çš„å½±ç‰‡è·¯å¾‘
process_video(video_path)  # é–‹å§‹é€å¹€è™•ç†å½±ç‰‡
