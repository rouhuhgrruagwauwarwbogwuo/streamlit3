import numpy as np
import streamlit as st
import cv2
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from PIL import Image
from mtcnn import MTCNN
import tempfile
import os
import requests

# æª¢æŸ¥ä¸¦ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ
def download_model():
    model_url = "https://huggingface.co/wuwuwu123123/deepfakemodel2/resolve/main/deepfake_cnn_model.h5"
    model_filename = "deepfake_cnn_model.h5"
    
    # å¦‚æœæ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œå‰‡ä¸‹è¼‰
    if not os.path.exists(model_filename):
        response = requests.get(model_url)
        if response.status_code == 200:
            with open(model_filename, "wb") as f:
                f.write(response.content)
            print("æ¨¡å‹æª”æ¡ˆå·²æˆåŠŸä¸‹è¼‰ï¼")
        else:
            print(f"ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            return None
    return model_filename

# ğŸ”¹ è¼‰å…¥ ResNet50 æ¨¡å‹
resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(256, 256, 3))
resnet_classifier = Sequential([
    resnet_model,
    Dense(1, activation='sigmoid')  # 1 å€‹è¼¸å‡ºç¯€é»ï¼ˆ0: çœŸå¯¦, 1: å‡ï¼‰
])
resnet_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ğŸ”¹ è¼‰å…¥è‡ªè¨‚ CNN æ¨¡å‹
model_path = download_model()
if model_path:
    custom_model = load_model(model_path)
else:
    custom_model = None

# ğŸ”¹ åˆå§‹åŒ– MTCNN äººè‡‰æª¢æ¸¬å™¨
detector = MTCNN()

# ğŸ”¹ é è™•ç†å‡½æ•¸ - é«˜é€šæ¿¾æ³¢ï¼ˆEdge Enhancementï¼‰
def high_pass_filter(img_array):
    kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])
    filtered_img = cv2.filter2D(img_array, -1, kernel)
    return filtered_img

# ğŸ”¹ é è™•ç†å‡½æ•¸ - é »åŸŸç‰¹å¾µåˆ†æ (FFT)
def fft_filter(img_array):
    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    f = np.fft.fft2(gray)
    fshift = np.fft.fftshift(f)
    magnitude_spectrum = np.log(np.abs(fshift) + 1)
    return magnitude_spectrum

# ğŸ”¹ é¡è‰²ç©ºé–“è½‰æ›
def convert_to_ycbcr(img_array):
    img_ycbcr = cv2.cvtColor(img_array, cv2.COLOR_RGB2YCrCb)
    return img_ycbcr

def convert_to_lab(img_array):
    img_lab = cv2.cvtColor(img_array, cv2.COLOR_RGB2LAB)
    return img_lab

# ğŸ”¹ CLAHE + éŠ³åŒ–é è™•ç†
def preprocess_image(image_path, target_size=(256, 256)):
    try:
        img = image.load_img(image_path, target_size=target_size)
        img_array = image.img_to_array(img).astype('uint8')

        # CLAHE (Contrast Limited Adaptive Histogram Equalization)
        img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        img_gray = clahe.apply(img_gray)

        # è½‰å› RGB
        img_array = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)
        img_array = img_array / 255.0  # æ¨™æº–åŒ–å½±åƒ (0~1)
        
        # é«˜é€šæ¿¾æ³¢å¢å¼·
        img_array = high_pass_filter(img_array)
        
        return np.expand_dims(img_array, axis=0)
    
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

# ğŸ”¹ äººè‡‰åµæ¸¬ï¼Œæ“·å–äººè‡‰å€åŸŸ
def extract_face(img):
    try:
        img_rgb = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)
        
        # ç¢ºä¿åœ–åƒå°ºå¯¸è¶³å¤ å¤§
        if img_rgb.shape[0] < 20 or img_rgb.shape[1] < 20:
            raise ValueError("åœ–åƒå°ºå¯¸éå°ï¼Œç„¡æ³•é€²è¡Œäººè‡‰æª¢æ¸¬")
        
        faces = detector.detect_faces(img_rgb)
        if len(faces) > 0:
            x, y, width, height = faces[0]['box']
            face = img_rgb[y:y+height, x:x+width]
            return Image.fromarray(face)
        else:
            print("æœªåµæ¸¬åˆ°äººè‡‰")
            return None
    except Exception as e:
        print(f"äººè‡‰åµæ¸¬éŒ¯èª¤: {e}")
        return None

# ğŸ”¹ é è™•ç†åœ–ç‰‡ï¼Œç¢ºä¿ ResNet å’Œ è‡ªè¨‚ CNN éƒ½èƒ½è™•ç†
def preprocess_for_both_models(image_path):
    try:
        img = image.load_img(image_path, target_size=(256, 256))  # èª¿æ•´å¤§å°
        img_array = image.img_to_array(img)
        
        # ResNet50 éœ€è¦ç‰¹åˆ¥çš„ preprocess_input
        resnet_input = preprocess_input(np.expand_dims(img_array, axis=0))
        
        # è‡ªè¨‚ CNN åªéœ€è¦æ­£è¦åŒ– (0~1)
        custom_input = np.expand_dims(img_array / 255.0, axis=0)
        
        return resnet_input, custom_input
    except Exception as e:
        print(f"åœ–ç‰‡è™•ç†éŒ¯èª¤: {e}")
        return None, None

# ğŸ”¹ é€²è¡Œé æ¸¬
def predict_with_both_models(image_path):
    resnet_input, custom_input = preprocess_for_both_models(image_path)
    
    if resnet_input is None or custom_input is None:
        return "è™•ç†éŒ¯èª¤", 0.0, "è™•ç†éŒ¯èª¤", 0.0
    
    # ResNet50 é æ¸¬
    resnet_prediction = resnet_classifier.predict(resnet_input)[0][0]
    resnet_label = "Deepfake" if resnet_prediction > 0.5 else "Real"
    
    # è‡ªè¨‚ CNN æ¨¡å‹é æ¸¬
    custom_prediction = custom_model.predict(custom_input)[0][0]
    custom_label = "Deepfake" if custom_prediction > 0.5 else "Real"
    
    return resnet_label, resnet_prediction, custom_label, custom_prediction

# ğŸ”¹ é¡¯ç¤ºåœ–ç‰‡å’Œé æ¸¬çµæœ
def show_prediction(image_path):
    resnet_label, resnet_confidence, custom_label, custom_confidence = predict_with_both_models(image_path)
    
    # é¡¯ç¤ºåœ–ç‰‡
    img = image.load_img(image_path, target_size=(256, 256))
    st.image(img, caption="é æ¸¬åœ–ç‰‡", use_column_width=True)
    
    # é¡¯ç¤ºé æ¸¬çµæœ
    st.subheader(f"ResNet50: {resnet_label} ({resnet_confidence:.2%})\n"
                 f"Custom CNN: {custom_label} ({custom_confidence:.2%})")

# ğŸ”¹ Streamlit ä¸»æ‡‰ç”¨ç¨‹å¼
st.set_page_config(page_title="Deepfake åµæ¸¬å™¨", layout="wide")
st.title("ğŸ§  Deepfake åœ–ç‰‡èˆ‡å½±ç‰‡åµæ¸¬å™¨")

tab1, tab2 = st.tabs(["ğŸ–¼ï¸ åœ–ç‰‡åµæ¸¬", "ğŸ¥ å½±ç‰‡åµæ¸¬"])

# ---------- åœ–ç‰‡ ---------- 
with tab1:
    st.header("åœ–ç‰‡åµæ¸¬")
    uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        pil_img = Image.open(uploaded_image).convert("RGB")
        st.image(pil_img, caption="åŸå§‹åœ–ç‰‡", use_column_width=True)

        # å˜—è©¦æ“·å–äººè‡‰å€åŸŸ
        face_img = extract_face(pil_img)
        if face_img:
            st.image(face_img, caption="åµæ¸¬åˆ°çš„äººè‡‰", use_column_width=True)
            show_prediction(face_img)
        else:
            st.write("æœªåµæ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´é«”åœ–ç‰‡é€²è¡Œé æ¸¬")
            show_prediction(uploaded_image)

# ---------- å½±ç‰‡ ---------- 
with tab2:
    st.header("å½±ç‰‡åµæ¸¬ï¼ˆæ¯ 10 å¹€æŠ½åœ–ï¼‰")
    uploaded_video = st.file_uploader("ä¸Šå‚³å½±ç‰‡", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_video.read())
            video_path = tmp.name

        st.info("ğŸ¬ æ“·å–å½±ç‰‡å¹€èˆ‡é€²è¡Œé æ¸¬ä¸­...")
        cap = cv2.VideoCapture(video_path)
        frame_idx = 0
        results = []
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            if frame_idx % 10 == 0:  # æ¯ 10 å¹€é€²è¡Œä¸€æ¬¡è™•ç†
                frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                face_img = extract_face(frame_pil)
                if face_img:
                    result = predict_with_both_models(face_img)
                    results.append((frame_idx, result))
                frame_idx += 1
        cap.release()

        # é¡¯ç¤ºå½±ç‰‡çµæœ
        for idx, (resnet_label, resnet_confidence, custom_label, custom_confidence) in results:
            st.image(frame_pil, caption=f"ç¬¬ {idx} å¹€ - {resnet_label} ({resnet_confidence:.2%})", use_column_width=True)
