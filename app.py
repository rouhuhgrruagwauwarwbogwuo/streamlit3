import numpy as np
import streamlit as st
from keras.applications import ResNet50
from keras.models import Sequential
from keras.layers import Dense
from PIL import Image
from mtcnn import MTCNN
import requests
import os
import cv2
import tempfile

# ğŸ”½ ä¸‹è¼‰æ¨¡å‹ï¼ˆå¦‚æœæ¨¡å‹æœªä¸‹è¼‰éï¼‰
def download_model():
    model_url = "https://huggingface.co/wuwuwu123123/deepfakemodel2/resolve/main/deepfake_cnn_model.h5"
    model_filename = "deepfake_cnn_model.h5"
    
    if not os.path.exists(model_filename):
        response = requests.get(model_url)
        if response.status_code == 200:
            with open(model_filename, "wb") as f:
                f.write(response.content)
            print("âœ… æ¨¡å‹æª”æ¡ˆå·²æˆåŠŸä¸‹è¼‰")
        else:
            print(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            return None
    return model_filename

# ğŸ”¹ è¼‰å…¥ ResNet50 æ¨¡å‹
try:
    resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    resnet_classifier = Sequential([
        resnet_model,
        Dense(1, activation='sigmoid')  
    ])
    resnet_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    print("âœ… ResNet50 æ¨¡å‹å·²æˆåŠŸè¼‰å…¥")
except Exception as e:
    print(f"âŒ è¼‰å…¥ ResNet50 æ¨¡å‹éŒ¯èª¤ï¼š{e}")
    resnet_classifier = None

# ğŸ”¹ åˆå§‹åŒ– MTCNN äººè‡‰æª¢æ¸¬å™¨
detector = MTCNN()

# ğŸ”¹ æ“·å–åœ–ç‰‡ä¸­çš„äººè‡‰
def extract_face(pil_img):
    img_array = np.array(pil_img)
    faces = detector.detect_faces(img_array)
    if len(faces) > 0:
        x, y, width, height = faces[0]['box']
        face = img_array[y:y+height, x:x+width]
        face_pil = Image.fromarray(face)
        return face_pil
    else:
        return None

# ğŸ”¹ ä¸­å¿ƒè£åˆ‡
def center_crop(img, target_size=(224, 224)):
    width, height = img.size
    new_width, new_height = target_size
    left = (width - new_width) // 2
    top = (height - new_height) // 2
    right = left + new_width
    bottom = top + new_height
    return img.crop((left, top, right, bottom))

# ğŸ”¹ åœ–ç‰‡é è™•ç†
def preprocess_for_resnet(img):
    img = img.resize((256, 256), Image.Resampling.LANCZOS)
    img = center_crop(img, (224, 224))
    img_array = np.array(img).astype(np.float32) / 255.0
    resnet_input = np.expand_dims(img_array, axis=0)
    return resnet_input

# ğŸ”¹ é æ¸¬
def predict_with_resnet(img):
    resnet_input = preprocess_for_resnet(img)
    prediction = resnet_classifier.predict(resnet_input)[0][0]
    label = "Deepfake" if prediction > 0.5 else "Real"
    return label, prediction

# ğŸ”¹ é¡¯ç¤ºé æ¸¬çµæœ
def show_prediction(img):
    label, confidence = predict_with_resnet(img)
    st.image(img, caption="è¼¸å…¥åœ–ç‰‡", use_container_width=True)
    st.subheader(f"ğŸ” é æ¸¬çµæœï¼š**{label}**ï¼ˆä¿¡å¿ƒå€¼ï¼š{confidence:.2%}ï¼‰")

# ğŸ”¹ Streamlit UI
st.set_page_config(page_title="Deepfake åµæ¸¬å™¨", layout="wide")
st.title("ğŸ§  Deepfake åœ–ç‰‡èˆ‡å½±ç‰‡åµæ¸¬å™¨")

tab1, tab2 = st.tabs(["ğŸ–¼ï¸ åœ–ç‰‡åµæ¸¬", "ğŸ¥ å½±ç‰‡åµæ¸¬"])

# ---------- åœ–ç‰‡ ----------
with tab1:
    st.header("åœ–ç‰‡åµæ¸¬")
    uploaded_image = st.file_uploader("è«‹ä¸Šå‚³åœ–ç‰‡ï¼ˆjpg/pngï¼‰", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        pil_img = Image.open(uploaded_image).convert("RGB")
        st.image(pil_img, caption="åŸå§‹åœ–ç‰‡", use_container_width=True)

        face_img = extract_face(pil_img)
        if face_img:
            st.image(face_img, caption="åµæ¸¬åˆ°çš„äººè‡‰", use_container_width=False, width=300)
            show_prediction(face_img)
        else:
            st.info("âš ï¸ æœªåµæ¸¬åˆ°äººè‡‰ï¼Œä½¿ç”¨æ•´å¼µåœ–ç‰‡é€²è¡Œåˆ†æ")
            show_prediction(pil_img)

# ---------- å½±ç‰‡ ----------
with tab2:
    st.header("å½±ç‰‡åµæ¸¬ï¼ˆæ¯ 10 å¹€åˆ†æä¸€æ¬¡ï¼‰")
    uploaded_video = st.file_uploader("è«‹ä¸Šå‚³å½±ç‰‡ï¼ˆmp4/mov/aviï¼‰", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_video.read())
            video_path = tmp.name

        st.info("ğŸ¬ æ“·å–å½±ç‰‡ä¸­... è«‹ç¨å€™")

        cap = cv2.VideoCapture(video_path)
        frame_idx = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_idx % 10 == 0:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_pil = Image.fromarray(frame_rgb)

                face_img = extract_face(frame_pil)
                if face_img:
                    st.image(face_img, caption=f"ç¬¬ {frame_idx} å¹€åµæ¸¬åˆ°äººè‡‰", use_container_width=False, width=300)
                    show_prediction(face_img)
                else:
                    st.image(frame_pil, caption=f"ç¬¬ {frame_idx} å¹€ï¼ˆæœªåµæ¸¬åˆ°äººè‡‰ï¼‰", use_container_width=True)
                    show_prediction(frame_pil)

            frame_idx += 1

        cap.release()
