import streamlit as st
import numpy as np
import os
import requests
from PIL import Image
import cv2
import tempfile
from tensorflow.keras.applications import ResNet50, EfficientNetB0, Xception
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # æ›´æ–°é€™è£¡
from mtcnn import MTCNN
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers

# â¬‡ï¸ ä¸‹è¼‰æ¨¡å‹ï¼ˆå¦‚æœé‚„æ²’ä¸‹è¼‰ï¼‰
def download_model():
    model_url = "https://huggingface.co/wuwuwu123123/deepfakemodel2/resolve/main/deepfake_cnn_model.h5"
    model_filename = "deepfake_cnn_model.h5"
    if not os.path.exists(model_filename):
        response = requests.get(model_url)
        if response.status_code == 200:
            with open(model_filename, "wb") as f:
                f.write(response.content)
            print("æ¨¡å‹ä¸‹è¼‰æˆåŠŸï¼")
        else:
            print(f"ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            return None
    return model_filename

# âœ… è¼‰å…¥å¤šå€‹é è¨“ç·´æ¨¡å‹
def load_models():
    # ResNet50æ¨¡å‹
    resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    resnet_classifier = Sequential([
        resnet_model,
        Dense(1, activation='sigmoid')
    ])
    resnet_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    
    # EfficientNetB0æ¨¡å‹
    efficientnet_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    efficientnet_classifier = Sequential([
        efficientnet_model,
        Dense(1, activation='sigmoid')
    ])
    efficientnet_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

    # Xceptionæ¨¡å‹
    xception_model = Xception(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))
    xception_classifier = Sequential([
        xception_model,
        Dense(1, activation='sigmoid')
    ])
    xception_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    
    return resnet_classifier, efficientnet_classifier, xception_classifier

# âœ… MTCNN åˆå§‹åŒ–
detector = MTCNN()

# âœ… æ“·å–äººè‡‰
def extract_face(pil_img):
    img_array = np.array(pil_img)
    faces = detector.detect_faces(img_array)
    if len(faces) > 0:
        x, y, w, h = faces[0]['box']
        face = img_array[y:y+h, x:x+w]
        return Image.fromarray(face)
    return None

# âœ… åœ–ç‰‡è™•ç†æ–¹æ³•
def apply_clahe(img):
    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])
    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)

def sharpen_image(img):
    kernel = np.array([[0, -1, 0],
                       [-1, 5,-1],
                       [0, -1, 0]])
    return cv2.filter2D(img, -1, kernel)

def high_pass_filter(img):
    blurred = cv2.GaussianBlur(img, (5,5), 0)
    return cv2.subtract(img, blurred)

# âœ… ä¸­å¿ƒè£åˆ‡
def center_crop(img, target_size=(224, 224)):
    width, height = img.size
    new_width, new_height = target_size
    left = (width - new_width) // 2
    top = (height - new_height) // 2
    return img.crop((left, top, left + new_width, top + new_height))

# âœ… é è™•ç†
def preprocess_for_model(pil_img):
    img = center_crop(pil_img, (224, 224))
    img_array = np.array(img)

    # âœ… åœ–åƒå¢å¼·
    img_array = apply_clahe(img_array)
    img_array = sharpen_image(img_array)
    img_array = high_pass_filter(img_array)

    img_array = img_array.astype(np.float32) / 255.0
    return np.expand_dims(img_array, axis=0)

# âœ… é æ¸¬
def predict_with_ensemble(img, models):
    pred_resnet = models[0].predict(preprocess_for_model(img), verbose=0)[0][0]
    pred_efficientnet = models[1].predict(preprocess_for_model(img), verbose=0)[0][0]
    pred_xception = models[2].predict(preprocess_for_model(img), verbose=0)[0][0]
    
    # æŠ•ç¥¨æˆ–åŠ æ¬Šå¹³å‡ï¼ˆåœ¨æ­¤ä½¿ç”¨ç°¡å–®çš„æŠ•ç¥¨æ–¹æ³•ï¼‰
    pred_avg = np.mean([pred_resnet, pred_efficientnet, pred_xception])
    
    label = "Deepfake" if pred_avg > 0.5 else "Real"
    confidence = pred_avg
    return label, confidence

# âœ… é¡¯ç¤ºé æ¸¬
def show_prediction(img, models):
    label, confidence = predict_with_ensemble(img, models)
    st.image(img, caption="è¼¸å…¥åœ–ç‰‡", use_container_width=True)
    st.subheader(f"é›†æˆå­¸ç¿’çµæœï¼š**{label}**ï¼ˆä¿¡å¿ƒåº¦ï¼š{confidence:.2%}ï¼‰")

# âœ… Streamlit UI
st.set_page_config(page_title="Deepfake åµæ¸¬å™¨", layout="wide")
st.title("ğŸ§  Deepfake åœ–ç‰‡åµæ¸¬å™¨")

tab1, tab2 = st.tabs(["ğŸ–¼ï¸ åœ–ç‰‡åµæ¸¬", "ğŸ¥ å½±ç‰‡åµæ¸¬"])

# âœ… è¼‰å…¥æ¨¡å‹
models = load_models()

# âœ… åœ–ç‰‡åµæ¸¬
with tab1:
    st.header("ä¸Šå‚³åœ–ç‰‡é€²è¡Œ Deepfake åµæ¸¬")
    uploaded_image = st.file_uploader("è«‹é¸æ“‡åœ–ç‰‡", type=["jpg", "jpeg", "png"])
    if uploaded_image:
        pil_img = Image.open(uploaded_image).convert("RGB")
        st.image(pil_img, caption="åŸå§‹åœ–ç‰‡", use_container_width=True)

        face_img = extract_face(pil_img)
        if face_img:
            st.image(face_img, caption="åµæ¸¬åˆ°äººè‡‰", width=300)
            show_prediction(face_img, models)
        else:
            st.info("âš ï¸ æœªåµæ¸¬åˆ°äººè‡‰ï¼Œå°‡ä½¿ç”¨æ•´å¼µåœ–ç‰‡é€²è¡Œé æ¸¬")
            show_prediction(pil_img, models)

# âœ… å½±ç‰‡åµæ¸¬ï¼ˆåƒ…æ“·å–å‰å¹¾å¹€ï¼‰
with tab2:
    st.header("å½±ç‰‡åµæ¸¬ï¼ˆåƒ…æ“·å–å‰å¹¾å¹€é€²è¡Œåˆ¤æ–·ï¼‰")
    uploaded_video = st.file_uploader("è«‹ä¸Šå‚³å½±ç‰‡", type=["mp4", "mov", "avi"])
    if uploaded_video:
        st.video(uploaded_video)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_video.read())
            video_path = tmp.name

        import cv2
        st.info("ğŸ¬ æ“·å–å¹€èˆ‡åˆ†æä¸­...")
        cap = cv2.VideoCapture(video_path)
        frame_idx = 0
        shown = False

        while cap.isOpened() and not shown:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_idx % 10 == 0:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_frame = Image.fromarray(rgb)
                face_img = extract_face(pil_frame)
                if face_img:
                    st.image(face_img, caption=f"ç¬¬ {frame_idx} å¹€åµæ¸¬åˆ°äººè‡‰", width=300)
                    show_prediction(face_img, models)
                    shown = True
            frame_idx += 1

        cap.release()
